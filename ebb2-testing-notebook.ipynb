{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import hashlib\n",
    "import itertools  # for reading the parameters file\n",
    "import sys  # for system errors and printouts\n",
    "from pathlib import Path  # for paths of files\n",
    "import os  # for reading the input data\n",
    "import time  # for timing\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "parameter_file = 'default_parameters.ini'  # the main parameters file\n",
    "# the main path where all the data directories are\n",
    "data_main_directory = Path('data')\n",
    "# dictionary that holds the input parameters, key = parameter name, value = value\n",
    "parameters_dictionary = dict()\n",
    "# dictionary of the input documents, key = document id, value = the document\n",
    "document_list = dict()\n",
    "\n",
    "\n",
    "# DO NOT CHANGE THIS METHOD\n",
    "# Reads the parameters of the project from the parameter file 'file'\n",
    "# and stores them to the parameter dictionary 'parameters_dictionary'\n",
    "def read_parameters():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(parameter_file)\n",
    "    for section in config.sections():\n",
    "        for key in config[section]:\n",
    "            if key == 'data':\n",
    "                parameters_dictionary[key] = config[section][key]\n",
    "            elif key == 'naive':\n",
    "                parameters_dictionary[key] = bool(config[section][key])\n",
    "            elif key == 't':\n",
    "                parameters_dictionary[key] = float(config[section][key])\n",
    "            else:\n",
    "                parameters_dictionary[key] = int(config[section][key])\n",
    "\n",
    "\n",
    "# DO NOT CHANGE THIS METHOD\n",
    "# Reads all the documents in the 'data_path' and stores them in the dictionary 'document_list'\n",
    "def read_data(data_path):\n",
    "    for (root, dirs, file) in os.walk(data_path):\n",
    "        for f in file:\n",
    "            file_path = data_path / f\n",
    "            doc = open(file_path).read().strip().replace('\\n', ' ')\n",
    "            file_id = int(file_path.stem)\n",
    "            document_list[file_id] = doc\n",
    "\n",
    "\n",
    "# DO NOT CHANGE THIS METHOD\n",
    "# Calculates the Jaccard Similarity between two documents represented as sets\n",
    "def jaccard(doc1, doc2):\n",
    "    return len(doc1.intersection(doc2)) / float(len(doc1.union(doc2)))\n",
    "\n",
    "\n",
    "# DO NOT CHANGE THIS METHOD\n",
    "# Define a function to map a 2D matrix coordinate into a 1D index.\n",
    "def get_triangle_index(i, j, length):\n",
    "    if i == j:  # that's an error.\n",
    "        sys.stderr.write(\"Can't access triangle matrix with i == j\")\n",
    "        sys.exit(1)\n",
    "    if j < i:  # just swap the values.\n",
    "        temp = i\n",
    "        i = j\n",
    "        j = temp\n",
    "\n",
    "    # Calculate the index within the triangular array. Taken from pg. 211 of:\n",
    "    # http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n",
    "    # adapted for a 0-based index.\n",
    "    k = int(i * (length - (i + 1) / 2.0) + j - i) - 1\n",
    "\n",
    "    return k\n",
    "\n",
    "\n",
    "# DO NOT CHANGE THIS METHOD\n",
    "# Calculates the similarities of all the combinations of documents and returns the similarity triangular matrix\n",
    "def naive():\n",
    "    docs_Sets = []  # holds the set of words of each document\n",
    "\n",
    "    for doc in document_list.values():\n",
    "        docs_Sets.append(set(doc.split()))\n",
    "\n",
    "    # Using triangular array to store the similarities, avoiding half size and similarities of i==j\n",
    "    num_elems = int(len(docs_Sets) * (len(docs_Sets) - 1) / 2)\n",
    "    similarity_matrix = [0 for x in range(num_elems)]\n",
    "    for i in range(len(docs_Sets)):\n",
    "        for j in range(i + 1, len(docs_Sets)):\n",
    "            similarity_matrix[get_triangle_index(i, j, len(docs_Sets))] = jaccard(\n",
    "                docs_Sets[i], docs_Sets[j])\n",
    "\n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reading...\n",
      "20 documents were read in 0.1324901580810547 sec\n",
      "\n",
      "Starting to calculate the similarities of documents...\n",
      "Calculating the similarities of 190 combinations of documents took 0.004998207092285156 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_parameters()\n",
    "#print(parameters_dictionary['data'])\n",
    "\n",
    "# Reading the data\n",
    "print(\"Data reading...\")\n",
    "data_folder = data_main_directory / parameters_dictionary['data']\n",
    "t0 = time.time()\n",
    "read_data(data_folder)\n",
    "document_list = {k: document_list[k] for k in sorted(document_list)}\n",
    "t1 = time.time()\n",
    "print(len(document_list), \"documents were read in\", t1 - t0, \"sec\\n\")\n",
    "\n",
    "# Naive\n",
    "naive_similarity_matrix = []\n",
    "if parameters_dictionary['naive']:\n",
    "    print(\"Starting to calculate the similarities of documents...\")\n",
    "    t2 = time.time()\n",
    "    naive_similarity_matrix = naive()\n",
    "    t3 = time.time()\n",
    "    print(\"Calculating the similarities of\", len(naive_similarity_matrix),\n",
    "            \"combinations of documents took\", t3 - t2, \"sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shingles():\n",
    "    docs_k_shingles = []  # holds the k-shingles of each document\n",
    "    # implement your code here\n",
    "\n",
    "    # Get the value k from the parameters dictionary\n",
    "    k = parameters_dictionary.get(\"k\")\n",
    "\n",
    "    for key in document_list:\n",
    "        document = document_list[key]\n",
    "        words = document.split()\n",
    "\n",
    "        shingles_in_doc = []\n",
    "        for i in range(len(words)-k+1):\n",
    "\n",
    "            shingle = words[i:i + k]\n",
    "\n",
    "            shingle = ' '.join(shingle)\n",
    "\n",
    "            if shingle not in shingles_in_doc:\n",
    "                shingles_in_doc.append(shingle)\n",
    "\n",
    "        docs_k_shingles.append(shingles_in_doc)\n",
    "\n",
    "    return docs_k_shingles\n",
    "\n",
    "\n",
    "docs_k_shingles = k_shingles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD FOR TASK 2\n",
    "# Creates a signatures set of the documents from the k-shingles list\n",
    "def signature_set(shingles):\n",
    "    # docs_sig_sets = []\n",
    "    # print(shingles)\n",
    "\n",
    "    docs_signature_sets = []\n",
    "    # print(\"len\", len(document_list))\n",
    "    for key in range(0, len(document_list)):\n",
    "        # print(\"key \", key)\n",
    "        # print(document_list[key])\n",
    "        # print(len(document_list))\n",
    "\n",
    "        shingle = shingles[key]\n",
    "        # print(\"shingle\", shingle)\n",
    "        signature_set_shingle = set()\n",
    "\n",
    "        # print(type(document_list))\n",
    "\n",
    "        # for shingle in document_list\n",
    "        # document = document_list[key+1]\n",
    "\n",
    "        # print(\"doc: \",document)\n",
    "        for i in range(len(shingles[key])):\n",
    "            hash_val = hash(shingle[i])\n",
    "            # print(hash_val)\n",
    "            if hash_val not in signature_set_shingle:\n",
    "                signature_set_shingle.add(hash_val)\n",
    "\n",
    "            # shing = shingles[key][i]\n",
    "            # print(shing in document)\n",
    "        # print(signature_set_shingle)\n",
    "        docs_signature_sets.append(signature_set_shingle)\n",
    "    return docs_signature_sets\n",
    "\n",
    "docs_signature_sets = signature_set(docs_k_shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sjekker hvis et tall er et prim tall\n",
    "def is_prime(n):\n",
    "    if n == 2 or n == 3:\n",
    "        return True\n",
    "    if n < 2 or n % 2 == 0:\n",
    "        return False\n",
    "    if n < 9:\n",
    "        return True\n",
    "    if n % 3 == 0:\n",
    "        return False\n",
    "    r = int(n**0.5)\n",
    "    # since all primes > 3 are of the form 6n Â± 1\n",
    "    # start with f=5 (which is prime)\n",
    "    # and test f, f+2 for being prime\n",
    "    # then loop by 6.\n",
    "    f = 5\n",
    "    while f <= r:\n",
    "        if n % f == 0:\n",
    "            return False\n",
    "        if n % (f+2) == 0:\n",
    "            return False\n",
    "        f += 6\n",
    "    return True\n",
    "\n",
    "random.seed(11)\n",
    "\n",
    "def minHash(document_vector):\n",
    "    num_permutations = parameters_dictionary.get(\"permutations\")\n",
    "    num_documents = len(document_list)\n",
    "\n",
    "    # Nye\n",
    "    signatures = np.full((num_documents, num_permutations), np.inf)\n",
    "    #Gammel \n",
    "    #signatures = np.full((num_permutations,num_documents), np.inf)\n",
    "    \n",
    "    count = 0\n",
    "    perms = 0\n",
    "    docies = 0\n",
    "\n",
    "    #Nye\n",
    "    for i in range(num_documents):\n",
    "    #Gammel\n",
    "    #for i in range(num_permutations):\n",
    "        a = random.randint(1, 400)\n",
    "        b = random.randint(1, 400)\n",
    "        cont = True\n",
    "        while cont:\n",
    "            # Generate a random number in the range [lower_bound, upper_bound]\n",
    "            p = random.randint(400, 1000)\n",
    "\n",
    "            # Check if the number is prime\n",
    "            if is_prime(p):\n",
    "                cont = False\n",
    "        perms += 1\n",
    "        #nye\n",
    "        for j in range(num_permutations):\n",
    "        #gammel\n",
    "        #for j in range(num_documents):\n",
    "            docies += 1\n",
    "            localcount = 1\n",
    "            for sig in document_vector[j]:\n",
    "\n",
    "                hash_value = (a * sig + b) % p\n",
    "                count += 1\n",
    "                localcount += 1\n",
    "\n",
    "                if hash_value < signatures[i][j]:\n",
    "                    signatures[i][j] = hash_value\n",
    "    return signatures\n",
    "\n",
    "signatures = minHash(docs_signature_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  1.  9. 12. 12. 10.  1.  0.  1.  2.]\n",
      " [ 1.  2. 10.  0.  0.  2.  2.  0.  5.  3.]\n",
      " [ 0.  4.  0.  4.  3.  6.  0.  1.  1. 12.]\n",
      " [ 3.  2.  1.  1.  6. 19.  0.  7.  0.  1.]\n",
      " [ 5.  1.  3.  5.  5. 11.  0.  0.  6.  6.]\n",
      " [ 2.  2.  3.  0.  0.  1. 12.  0.  7.  3.]\n",
      " [ 3.  2.  3.  0.  0.  0.  3.  2.  2.  0.]\n",
      " [ 0.  0.  2.  0.  1.  3.  0.  2.  6.  0.]\n",
      " [ 1.  0.  1.  1.  1.  0.  0.  2.  1.  0.]\n",
      " [ 0.  1.  0.  0.  4.  2.  2.  0.  2.  0.]\n",
      " [ 6.  1.  1.  2.  0.  3.  0.  1.  0.  4.]\n",
      " [ 0.  6.  6.  0.  0.  2.  0.  0.  5.  0.]\n",
      " [ 1.  0.  1.  1.  0. 14.  0.  0.  1.  2.]\n",
      " [ 1.  1.  1.  1.  1.  2.  1.  0.  0.  2.]\n",
      " [ 0.  3.  7.  2.  4.  3.  1.  5.  0.  4.]\n",
      " [ 1.  1.  7.  0.  3.  4.  0.  0.  1.  5.]\n",
      " [ 0.  1.  6.  9.  8. 13.  3.  2.  0.  0.]\n",
      " [ 0.  1.  1.  0.  1.  0.  0.  1.  1.  2.]\n",
      " [ 2.  0.  1.  1.  0.  3.  0.  0.  0.  2.]\n",
      " [ 3.  2.  2.  3.  0.  0.  1.  0.  2.  7.]]\n"
     ]
    }
   ],
   "source": [
    "#print(signaturesGammel)\n",
    "#print(signaturesNy)\n",
    "print(signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n#print(signaturesNy)\\ntemplis =[]\\ntempcols = []\\nfor i in signaturesNy:\\n    templis.append(i[0])\\nfor j in signaturesGammel:\\n    tempcols.append(j[0])\\nprint(templis)\\nprint(tempcols)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "#print(signaturesNy)\n",
    "templis =[]\n",
    "tempcols = []\n",
    "for i in signaturesNy:\n",
    "    templis.append(i[0])\n",
    "for j in signaturesGammel:\n",
    "    tempcols.append(j[0])\n",
    "print(templis)\n",
    "print(tempcols)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD FOR TASK 4\n",
    "# Hashes the MinHash Signature Matrix into buckets and find candidate similar documents\n",
    "def lsh(m_matrix):\n",
    "\n",
    "    candidates = []  # list of candidate sets of documents for checking similarity\n",
    "    #print(m_matrix.T)\n",
    "    #m_matrix = m_matrix.T\n",
    "    # implement your code here\n",
    "    buckets = parameters_dictionary.get(\"buckets\")\n",
    "    # permutations = parameters_dictionary.get(\"permutations\")\n",
    "    # bands = len(m_matrix[0])/buckets\n",
    "    # print(bands)\n",
    "    # for signature in m_matrix:\n",
    "    bucket_dict = {}\n",
    "    for i in range(buckets):\n",
    "        bucket_dict[i] = set()\n",
    "\n",
    "    #print(\"shape \",m_matrix.shape[0])\n",
    "    # Hash each column of the signature matrix into a bucket\n",
    "    for j in range(m_matrix.shape[0]):\n",
    "        #hash_val = hash(tuple(m_matrix[:, j]))\n",
    "        hash_val = hash(tuple(m_matrix[j, :]))\n",
    "        bucket_idx = hash_val % buckets\n",
    "        bucket_dict[bucket_idx].add(j)\n",
    "\n",
    "    # Find candidate similar documents from the buckets\n",
    "    candidates = set()\n",
    "    for bucket in bucket_dict.values():\n",
    "        #print(len(bucket))\n",
    "        if len(bucket) > 1:\n",
    "            pairs = list(itertools.combinations(bucket, 2))\n",
    "            #print(pairs)\n",
    "            candidates.update(pairs)\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = lsh(signatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(3, 7), (4, 12), (17, 3), (8, 18), (19, 15), (10, 12), (1, 9), (0, 11), (18, 13), (4, 11), (10, 11), (0, 4), (0, 10), (4, 10), (17, 7), (5, 14), (8, 13), (0, 12), (11, 12), (2, 6), (16, 5), (16, 14)}\n",
      "[[ 2.  1.  9. 12. 12. 10.  1.  0.  1.  2.]\n",
      " [ 1.  2. 10.  0.  0.  2.  2.  0.  5.  3.]\n",
      " [ 0.  4.  0.  4.  3.  6.  0.  1.  1. 12.]\n",
      " [ 3.  2.  1.  1.  6. 19.  0.  7.  0.  1.]\n",
      " [ 5.  1.  3.  5.  5. 11.  0.  0.  6.  6.]\n",
      " [ 2.  2.  3.  0.  0.  1. 12.  0.  7.  3.]\n",
      " [ 3.  2.  3.  0.  0.  0.  3.  2.  2.  0.]\n",
      " [ 0.  0.  2.  0.  1.  3.  0.  2.  6.  0.]\n",
      " [ 1.  0.  1.  1.  1.  0.  0.  2.  1.  0.]\n",
      " [ 0.  1.  0.  0.  4.  2.  2.  0.  2.  0.]\n",
      " [ 6.  1.  1.  2.  0.  3.  0.  1.  0.  4.]\n",
      " [ 0.  6.  6.  0.  0.  2.  0.  0.  5.  0.]\n",
      " [ 1.  0.  1.  1.  0. 14.  0.  0.  1.  2.]\n",
      " [ 1.  1.  1.  1.  1.  2.  1.  0.  0.  2.]\n",
      " [ 0.  3.  7.  2.  4.  3.  1.  5.  0.  4.]\n",
      " [ 1.  1.  7.  0.  3.  4.  0.  0.  1.  5.]\n",
      " [ 0.  1.  6.  9.  8. 13.  3.  2.  0.  0.]\n",
      " [ 0.  1.  1.  0.  1.  0.  0.  1.  1.  2.]\n",
      " [ 2.  0.  1.  1.  0.  3.  0.  0.  0.  2.]\n",
      " [ 3.  2.  2.  3.  0.  0.  1.  0.  2.  7.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(candidates)\n",
    "print(signatures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "print(list(candidates)[0][0])\n",
    "pair = \n",
    "i = 0\n",
    "jac = jaccard(set(signatures[list(candidates)[i][0]]),set(signatures[list(candidates)[i][1]]))\n",
    "print(jac)\n",
    "#print(signatures[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" similarity(d1, d2) = #(hi(d1) == hi(d2))\n",
    "permutations\n",
    "\"\"\"\n",
    "# METHOD FOR TASK 5\n",
    "# Calculates the similarities of the candidate documents\n",
    "def candidates_similarities(candidate_docs, min_hash_matrix):\n",
    "    similarity_matrix = []\n",
    "    #print(\"candidates \", candidate_docs)\n",
    "    #print(\"\\n\\n Min hash\", min_hash_matrix)\n",
    "    #jaccard(doc1, doc2)\n",
    "    for pair in candidate_docs:  \n",
    "            \n",
    "        index1 = pair[0] \n",
    "        index2 = pair[1] \n",
    "        jac = jaccard(set(min_hash_matrix[index1]),set(min_hash_matrix[index2]))\n",
    "        #print(\"jac \", jac)\n",
    "        similarity_matrix.append([index1, index2, jac])\n",
    "\n",
    "    \"\"\"\n",
    "    similarity(d1, d2) = # (hi(d1)) == hi(d2)) / permutations\n",
    "    similarity(d1, d2) = antall ord i d1 og i d2 /permutations\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # implement your code here\n",
    "    return similarity_matrix\n",
    "\n",
    "sim_matrix = candidates_similarities(candidates, signatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 7, 0.7142857142857143], [4, 12, 0.25], [17, 3, 0.42857142857142855], [8, 18, 0.75], [19, 15, 0.5714285714285714], [10, 12, 0.42857142857142855], [1, 9, 0.42857142857142855], [0, 11, 0.25], [18, 13, 0.75], [4, 11, 0.42857142857142855], [10, 11, 0.42857142857142855], [0, 4, 0.2], [0, 10, 0.3333333333333333], [4, 10, 0.5], [17, 7, 0.6], [5, 14, 0.625], [8, 13, 1.0], [0, 12, 0.42857142857142855], [11, 12, 0.3333333333333333], [2, 6, 0.2857142857142857], [16, 5, 0.4], [16, 14, 0.36363636363636365]]\n"
     ]
    }
   ],
   "source": [
    "print(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id1:  3  Id2:  7  similarity:  0.7142857142857143\n",
      "Id1:  8  Id2:  18  similarity:  0.75\n",
      "Id1:  18  Id2:  13  similarity:  0.75\n",
      "Id1:  5  Id2:  14  similarity:  0.625\n",
      "Id1:  8  Id2:  13  similarity:  1.0\n",
      "Total above threshold  0.6  =  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3, 7], [8, 18], [18, 13], [5, 14], [8, 13]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# METHOD FOR TASK 6\n",
    "# Returns the document pairs of over t% similarity\n",
    "#sim_matrix\n",
    "\n",
    "\"\"\"\n",
    "After calculating the similarity for the candidate document pairs from the previous\n",
    "task, count how many document pairs have similarity over the threshold input parameter t and print their ids. Also, count how many of the pairs are false positive and\n",
    "false negative using the naive method for the accurate similarity.\n",
    "\"\"\"\n",
    "def return_results(lsh_similarity_matrix):\n",
    "    document_pairs = []\n",
    "    t = parameters_dictionary['t']\n",
    "    count = 0\n",
    "    for pair in lsh_similarity_matrix:\n",
    "        threshold = pair[2]\n",
    "        if threshold > t:\n",
    "            count += 1\n",
    "            id1 = pair[0]\n",
    "            id2 = pair[1]\n",
    "            print(\"Id1: \", id1, \" Id2: \", id2, \" similarity: \", threshold)\n",
    "            document_pairs.append([id1, id2])\n",
    "        #print(pair[2])\n",
    "\n",
    "    # implement your code here\n",
    "\n",
    "    print(\"Total above threshold \", t, \" :\", count)\n",
    "    return document_pairs\n",
    "\n",
    "\n",
    "return_results(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# METHOD FOR TASK 6\n",
    "def count_false_neg_and_pos(lsh_similarity_matrix, naive_similarity_matrix):\n",
    "    false_negatives = 0\n",
    "    false_positives = 0\n",
    "\n",
    "    # implement your code here\n",
    "\n",
    "    return false_negatives, false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
